#!/bin/bash
#SBATCH -p gpu
#SBATCH -N 1 -c 32
#SBATCH --gpus-per-node=4
#SBATCH --ntasks-per-node=1
#SBATCH -t 02:00:00
#SBATCH -A lt200344
#SBATCH -J fastapi_ollama


ml Mamba
mamba activate fastapi

# export port=$(shuf -i 6000-9999 -n 1)
export port=8000
export USER=$(whoami)
export node=$(hostname -s)


echo "Assigned port: $port"
echo "User: $USER"
echo "Node: $node"



export OLLAMA_HOST=0.0.0.0:11434
export OLLAMA_MODELS=/project/lt200344-zhthmt/Y/OLLAMA_v0.5.7/models
export SCRATCH_BASE=/scratch/shared/$USER
export OLLAMA_MODEL=qwen3:14b
export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/project/lt200344-zhthmt/Y/OLLAMA/lib
export PATH=${PATH}:/project/lt200344-zhthmt/Y/OLLAMA/bin


echo "OLLAMA_GAME_PORT: $OLLAMA_GAME_PORT"
echo "OLLAMA_HOST: $OLLAMA_HOST"
echo "OLLAMA_MODELS: $OLLAMA_MODELS"
echo "OLLAMA_MODEL: $OLLAMA_MODEL"
echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH"
echo "PATH: $PATH"


ollama serve & 

sleep 10 
echo -e "ssh -L ${port}:${node}:${port} ${USER}@lanta.nstda.or.th -i id_rsa"
echo "FastAPI service is running on ${node}:${port}"
echo "${node}"

uvicorn main:app --host 0.0.0.0 --port ${port}

sleep 15


